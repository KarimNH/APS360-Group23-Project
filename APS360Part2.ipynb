{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUyIk3VhAvmN",
        "outputId": "174bc4a7-b097-4aab-9f62-0b4b6f378b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import random\n",
        "import seaborn as sns\n",
        "use_cuda=torch.cuda.is_available()\n",
        "\n",
        "print(use_cuda)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIqbEWW-BLcZ",
        "outputId": "e55df42e-ad14-4350-de33-c462e1c223d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gzip\n",
        "with gzip.open('/content/gdrive/MyDrive/APS360 Project DATA/MyDataFeatureFull.pkl', 'rb') as f: #\\\\MyDataFeatureFull\n",
        "    loaded_data = pickle.load(f)"
      ],
      "metadata": {
        "id": "tycYeUp7Az54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEG_CNN(nn.Module)\n",
        "    def __init__(self, name=\"EEG_CNN\"):\n",
        "        super(EEG_CNN,self).__init__()\n",
        "\n",
        "        self.name=name\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size = (3, 3), padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size = (3, 3), padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # Adjust dimensions for fully connected layers\n",
        "        self.fc1 = nn.Linear(64*5*2, 10)  # self.fc1 = nn.Linear(64*5*2, 10)\n",
        "        self.fc2 = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x))) # output dimensions: [32, 11, 4]\n",
        "        # print(x.size())\n",
        "        x = self.pool(F.relu(self.conv2(x))) # output dimensions: [64, 5, 2]\n",
        "        # print(x.size())\n",
        "        x = x.view(-1, 64*5*2) # flatten layer x.view(-1, 64*5*2) (-1, 64,23,9) x.view(x.size(0), -1)\n",
        "        # print(x.size())\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        # print(x.size())\n",
        "        return x"
      ],
      "metadata": {
        "id": "nE-dGPtXA05X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "1008c6cd-3724-4262-d556-18aff0156a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-34feb2db9862>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class EEG_CNN(nn.Module)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ANN_MNISTClassifier(nn.Module):\n",
        "    def __init__(self, name=\"EEG_CNN\"):\n",
        "        super(ANN_MNISTClassifier, self).__init__()\n",
        "\n",
        "        self.name=name\n",
        "\n",
        "        self.fc1 = nn.Linear(23*9, 50)\n",
        "        self.fc2 = nn.Linear(50, 20)\n",
        "        self.fc3 = nn.Linear(20, 2)\n",
        "\n",
        "    def forward(self, img):\n",
        "        flattened = img.view(-1, 23*9)\n",
        "        activation1 = F.relu(self.fc1(flattened))\n",
        "        activation2 = F.relu(self.fc2(activation1))\n",
        "        output = self.fc3(activation2)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Q9HqFSE_A5VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEG_CNN2(nn.Module):\n",
        "    def __init__(self, name=\"EEG_CNN2\"):\n",
        "        super(EEG_CNN2,self).__init__()\n",
        "\n",
        "        self.name=name\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size = (3, 3), padding=(1))\n",
        "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization after conv1\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size = (3, 3), padding=(1))\n",
        "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization after conv2\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size = (3, 3), padding=(1))  # Additional convolutional layer\n",
        "        self.bn3 = nn.BatchNorm2d(128)  # Batch normalization after conv3\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.dropout = nn.Dropout(0.3)  # Dropout for regularization\n",
        "\n",
        "        # Adjust dimensions for fully connected layers\n",
        "        # Update dimensions based on the output size from the conv layers\n",
        "        self.fc1 = nn.Linear(128*2*1, 64)  # Adjusted based on conv3 and pooling layers\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 2)  # Additional fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Add batch norm after conv1\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Add batch norm after conv2\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Add conv3 and batch norm\n",
        "\n",
        "        x = x.view(-1, 128*2*1)  # Flatten layer, adjust based on conv3 and pooling layers\n",
        "        x = self.dropout(F.relu(self.fc1(x)))  # Add dropout after activation\n",
        "        x = self.dropout(F.relu(self.fc2(x)))  # Add dropout after activation\n",
        "\n",
        "        x = self.fc3(x)  # Additional fully connected layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "DalYTBmTA6LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EEG_CNN3(nn.Module):\n",
        "    def __init__(self, name=\"EEG_CNN3\"):\n",
        "        super(EEG_CNN3,self).__init__()\n",
        "\n",
        "        self.name=name\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size = (3, 3), padding=(1))\n",
        "        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization after conv1\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size = (3, 3), padding=(1))\n",
        "        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization after conv2\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size = (3, 3), padding=(1))  # Additional convolutional layer\n",
        "        self.bn3 = nn.BatchNorm2d(128)  # Batch normalization after conv3\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # Adjust dimensions for fully connected layers\n",
        "        # Update dimensions based on the output size from the conv layers\n",
        "        self.fc1 = nn.Linear(128*2*1, 64)  # Adjusted based on conv3 and pooling layers\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 2)  # Additional fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Add batch norm after conv1\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Add batch norm after conv2\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Add conv3 and batch norm\n",
        "\n",
        "        x = x.view(-1, 128*2*1)  # Flatten layer, adjust based on conv3 and pooling layers\n",
        "\n",
        "        x = F.relu(self.fc1(x))  # Add dropout after activation\n",
        "        x = F.relu(self.fc2(x))  # Add dropout after activation\n",
        "\n",
        "        x = self.fc3(x)  # Additional fully connected layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "8DEaghwoPqQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(10)\n",
        "def trim_channels(lst, max_channels=23):\n",
        "    trimmed_lst = []\n",
        "    for data, label in lst:\n",
        "        # If data contains more than max_channels channels, trim it\n",
        "        if len(data) > max_channels:\n",
        "            data = data[:max_channels]\n",
        "        if len(data) == max_channels:\n",
        "            trimmed_lst.append((data, label))\n",
        "    return trimmed_lst\n",
        "\n",
        "loaded_data = trim_channels(loaded_data)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Extract labels only for stratified splitting\n",
        "labels = [label for data, label in loaded_data]\n",
        "# Convert labels to binary (0 = False, 1 = True) for stratification\n",
        "binary_labels = [1 if label else 0 for label in labels]\n",
        "# Perform stratified splitting\n",
        "train_data, temp_data = train_test_split(loaded_data, stratify=binary_labels, test_size=0.4)\n",
        "# Now split the temporary set into equal halves of validation and test sets.\n",
        "binary_labels_temp = [1 if label else 0 for data, label in temp_data]\n",
        "val_data, test_data = train_test_split(temp_data, stratify=binary_labels_temp, test_size=0.5)\n",
        "\n",
        "from sklearn.utils import resample\n",
        "# separate the True and False samples in train_data\n",
        "true_setT = [data for data in train_data if data[1] == True]\n",
        "false_setT = [data for data in train_data if data[1] == False]\n",
        "# oversample True class in train_data\n",
        "true_setT_oversampled = resample(true_setT, replace=True, n_samples=len(false_setT), random_state=123)\n",
        "# undersample False class in train_data\n",
        "false_setT_undersampled = resample(false_setT, replace=False, n_samples=len(true_setT_oversampled), random_state=123)\n",
        "# combine them to create a balanced train dataset\n",
        "balanced_train = true_setT_oversampled + false_setT_undersampled\n",
        "# shuffle the data\n",
        "random.shuffle(balanced_train)\n",
        "# separate the True and False samples in val_data\n",
        "true_setV = [data for data in val_data if data[1] == True]\n",
        "false_setV = [data for data in val_data if data[1] == False]\n",
        "# oversample True class in val_data\n",
        "true_setV_oversampled = resample(true_setV, replace=True, n_samples=len(false_setV), random_state=123)\n",
        "# undersample False class in val_data\n",
        "false_setV_undersampled = resample(false_setV, replace=False, n_samples=len(true_setV_oversampled), random_state=123)\n",
        "# combine them to create a balanced validation dataset\n",
        "balanced_val = true_setV_oversampled + false_setV_undersampled\n",
        "# shuffle the data\n",
        "random.shuffle(balanced_val)\n",
        "print(len(false_setT_undersampled))\n",
        "print(len(true_setT_oversampled))\n",
        "\n",
        "# Normalizing the data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def minmax_scale_data(data):\n",
        "    \"\"\"Scale the features in a dataset to a range between 0 and 1.\"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    return [(scaler.fit_transform(features), label) for features, label in data]\n",
        "\n",
        "# Apply the MinMax scaling\n",
        "train_dataN = minmax_scale_data(balanced_train)\n",
        "val_dataN = minmax_scale_data(balanced_val)\n",
        "test_dataN = minmax_scale_data(test_data)\n",
        "\n",
        "\n",
        "def get_metrics(model, data, batch_size):\n",
        "    torch.manual_seed(1)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    tp = 0  # True positives\n",
        "    fp = 0  # False positives\n",
        "    fn = 0  # False negatives\n",
        "    tn = 0  # True negatives\n",
        "    counter = 0\n",
        "\n",
        "\n",
        "    for imgs, labels in torch.utils.data.DataLoader(data, batch_size=batch_size):\n",
        "        imgs = imgs.unsqueeze(1)\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            imgs = imgs.cuda()\n",
        "            labels = labels.cuda()\n",
        "            model = model.cuda()\n",
        "        output = model(imgs.float())\n",
        "        counter +=1\n",
        "\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        pred = pred.squeeze()\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "\n",
        "        tp += ((pred == 1) & (labels == 1)).sum().item()\n",
        "        fp += ((pred == 1) & (labels == 0)).sum().item()\n",
        "        fn += ((pred == 0) & (labels == 1)).sum().item()\n",
        "        tn += ((pred == 0) & (labels == 0)).sum().item()\n",
        "\n",
        "\n",
        "    accuracy = correct / total\n",
        "    precision = tp / (tp + fp) if tp + fp != 0 else 0.0\n",
        "    recall = tp / (tp + fn) if tp + fn != 0 else 0.0\n",
        "    f1 = 2*((recall*precision)/(recall+precision)) if recall + precision != 0 else 0.0\n",
        "    confusion_matrix = torch.tensor([[tp, fp], [fn, tn]])\n",
        "\n",
        "    return accuracy, recall, precision, f1, confusion_matrix\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y8PT9UfA7EY",
        "outputId": "eac52b8c-7519-42c6-b0c3-faef38fd0e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38910\n",
            "38910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_data, val_data, batch_size, num_epochs, learning_rate=0.01, MASTER_SAVE=True):\n",
        "    torch.manual_seed(1)\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    criterion = nn.CrossEntropyLoss()  #criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    epoch = 0\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    counter = 0\n",
        "    epochs, iters, losses, train_acc, val_acc = [], [], [], [], []\n",
        "    ct = 0\n",
        "    n = 0\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    # Current model's metrics - validation\n",
        "    highest_accuracy = -1.0\n",
        "    highest_precision = -1.0\n",
        "    highest_recall = -1.0\n",
        "    highest_f1 = -1.0\n",
        "\n",
        "\n",
        "    best_paths = [' ',' ',' ',' ']\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        random.seed(1)\n",
        "        random.shuffle(train_data)\n",
        "        random.shuffle(val_data)\n",
        "        tp = 0  # True positives\n",
        "        fp = 0  # False positives\n",
        "        fn = 0  # False negatives\n",
        "        tn = 0  # True negatives\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "        for imgs, labels in iter(train_loader):\n",
        "            imgs = imgs.unsqueeze(1)\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            counter +=1\n",
        "            labels = labels.long()  # Change labels to long\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "                model.cuda()\n",
        "            else:\n",
        "                device = torch.device(\"cpu\")\n",
        "\n",
        "            out = model(imgs.float())\n",
        "            loss = criterion(out, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)\n",
        "            n += 1\n",
        "\n",
        "            # print((out[0][0]))\n",
        "            # print(len(out[0]))\n",
        "            # pred = out.max(1, keepdim=True)[1]\n",
        "            # # for i in range(len(pred)):\n",
        "            # #     tp += ((pred[i][0] == 1) & (labels[i] == 1))\n",
        "            # #     fp += ((pred[i][0] == 1) & (labels[i] == 0))\n",
        "            # #     fn += ((pred[i][0] == 0) & (labels[i] == 1))\n",
        "            # #     tn += ((pred[i][0] == 0) & (labels[i] == 0))\n",
        "            # # print((pred == 1)[0])\n",
        "            # pred = pred.squeeze()\n",
        "            # # print(pred)\n",
        "            # # print(1,(pred)[1])\n",
        "            # # print(2,(labels)[1])\n",
        "            # # print((pred)[0]==(labels)[0])\n",
        "            # # print((pred[0] == 1) & (labels[0] == 1))\n",
        "            # tp += ((pred == 1) & (labels == 1)).sum().item()\n",
        "            # fp += ((pred == 1) & (labels == 0)).sum().item()\n",
        "            # fn += ((pred == 0) & (labels == 1)).sum().item()\n",
        "            # tn += ((pred == 0) & (labels == 0)).sum().item()\n",
        "            # print(tn)\n",
        "            # accuracyT, recallT, precisionT, f1T = get_metrics(model, train_data, batch_size=batch_size)\n",
        "            # train_acc.append(accuracyT)\n",
        "            # accuracyV, recallV, precisionV, f1V = get_metrics(model, val_data, batch_size=batch_size)\n",
        "            # val_acc.append(accuracyV)\n",
        "        print(epoch)\n",
        "        epochs.append(epoch)\n",
        "        epoch +=1\n",
        "        accuracyT, recallT, precisionT, f1T, confusion_matrixT  = get_metrics(model, train_data, batch_size=batch_size)\n",
        "        train_acc.append(accuracyT)\n",
        "        accuracyV, recallV, precisionV, f1V, confusion_matrixV = get_metrics(model, val_data, batch_size=batch_size)\n",
        "        val_acc.append(accuracyV)\n",
        "\n",
        "        # Update models 'highest metrics'\n",
        "        highest_accuracy = (highest_accuracy, accuracyV)[accuracyV > highest_accuracy]\n",
        "        highest_precision = (highest_precision, precisionV)[precisionV > highest_precision]\n",
        "        highest_recall = (highest_recall, recallV)[recallV > highest_recall]\n",
        "        highest_f1 = (highest_f1, f1V)[f1V > highest_f1]\n",
        "\n",
        "        do_save, reason = determine_save(accuracyV, recallV, precisionV, f1V,\n",
        "                                         highest_accuracy, highest_precision,\n",
        "                                         highest_recall, highest_f1)\n",
        "\n",
        "\n",
        "        if(MASTER_SAVE and do_save):\n",
        "          model_path = get_model_name(model.name, batch_size,learning_rate, accuracyV, recallV, precisionV, f1V, epoch, reason)\n",
        "          best_paths[reason] = model_path\n",
        "          save_model(model, model_path)\n",
        "\n",
        "    print(n)\n",
        "        # print(epoch)\n",
        "    plt.title('Training Curve (' + str(model.name) + ')')\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "    print(len(epochs))\n",
        "    print(len(train_acc))\n",
        "    plt.title('Training Curve (' + str(model.name) + ')')\n",
        "    plt.plot(epochs, train_acc, label=\"Train\")\n",
        "    plt.plot(epochs, val_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Training Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    # print(tp+fp+fn+tn)\n",
        "    # confusion_matrix = torch.tensor([[tp, fp], [fn, tn]])\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(confusion_matrixT.cpu(), annot=True, cmap='Blues')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('Actual Labels')\n",
        "    plt.title('Confusion Matrix for Validation Data (' + str(model.name) + ')')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n",
        "    print(\"Final Training Accuracy/recall/precision/f1:\", accuracyT, recallT, precisionT, f1T)\n",
        "    print(\"Final Validation Accuracy/recall/precision/f1:\", accuracyV, recallV, precisionV, f1V)\n",
        "\n",
        "    print(\"\\nHighest of each metric:\")\n",
        "    print(\"USE THESE TO UPDATE get_save_metrics (copy paste)\")\n",
        "    print(\"save_accuracy = \" + str(highest_accuracy))\n",
        "    print(\"save_precision = \" + str(highest_precision))\n",
        "    print(\"save_recall = \" + str(highest_recall))\n",
        "    print(\"save_f1 = \" + str(highest_f1))\n",
        "\n",
        "    print(\"\\nCorresponding Model Names: \")\n",
        "    print(\"DOWNLOAD THESE -> BEST YOU HAVE SO FAR\")\n",
        "    print(\"Accuracy: \" + str(best_paths[0]))\n",
        "    print(\"Precision: \" + str(best_paths[1]))\n",
        "    print(\"Recall: \" + str(best_paths[2]))\n",
        "    print(\"F1: \" + str(best_paths[3]))"
      ],
      "metadata": {
        "id": "FPs1xjLfA-fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MUST UPDATE THESE AFTER EACH TRAINING MODEL\n",
        "\n",
        "# PASTE HERE\n",
        "def get_save_metrics():\n",
        "\n",
        "  save_recall = 0.9414803392444102\n",
        "  save_accuracy = 0.9459907478797225\n",
        "  save_precision = 0.9858392712185442\n",
        "  save_f1 = 0.9451298319821407\n",
        "\n",
        "  return save_accuracy, save_precision, save_recall, save_f1\n",
        "\n",
        "def determine_save(accuracy, recall, precision, f1,\n",
        "                                         highest_accuracy, highest_precision,\n",
        "                                         highest_recall, highest_f1):\n",
        "  save_accuracy, save_precision, save_recall, save_f1 = get_save_metrics()\n",
        "  do_save = False\n",
        "\n",
        "  save_accuracy = (save_accuracy, highest_accuracy)[highest_accuracy > save_accuracy]\n",
        "  save_precision = (save_precision, highest_precision)[highest_precision > save_precision]\n",
        "  save_recall = (save_recall, highest_recall)[highest_recall > save_recall]\n",
        "  save_f1 = (save_f1, highest_f1)[highest_f1 > save_f1]\n",
        "\n",
        "  if(f1 >= save_f1):\n",
        "    do_save = True\n",
        "    return do_save, 3\n",
        "\n",
        "  if(precision >= save_precision):\n",
        "    do_save = True\n",
        "    return do_save, 1\n",
        "\n",
        "  if(recall >= save_recall):\n",
        "    do_save = True\n",
        "    return do_save, 2\n",
        "\n",
        "  if(accuracy >= save_accuracy):\n",
        "    do_save = True\n",
        "    return do_save, 0\n",
        "\n",
        "  return False, ' '\n",
        "\n",
        "\n",
        "def save_model(model, path):\n",
        "  torch.save(model.state_dict(), path)\n",
        "\n",
        "\n",
        "def get_model_name(name, batch_size, learning_rate, accuracy, recall, precision, f1, epoch, basis):\n",
        "\n",
        "  accuracy = accuracy * 100\n",
        "  recall = recall * 100\n",
        "  precision = precision * 100\n",
        "  f1 = f1 * 100\n",
        "\n",
        "  # basis = basis[0].upper()\n",
        "\n",
        "  word = ' '\n",
        "\n",
        "  match basis:\n",
        "    case 0:\n",
        "      word = 'accuracy'\n",
        "    case 1: word = 'precision'\n",
        "    case 2: word = 'recall'\n",
        "    case 3: word = 'f1'\n",
        "    case _: word = 'error'\n",
        "\n",
        "  path = \"{reason}_{0}_bs{1}_lr{2}_a{3:.2f}_r{4:.2f}_p{5:.2f}_f{6:.2f}_epoch{7}\".format(\n",
        "                                  name,\n",
        "                                  batch_size,\n",
        "                                  learning_rate,\n",
        "                                  accuracy,\n",
        "                                  recall,\n",
        "                                  precision,\n",
        "                                  f1,\n",
        "                                  epoch,\n",
        "                                  reason=word)\n",
        "\n",
        "  return path\n",
        "\n"
      ],
      "metadata": {
        "id": "-M-CusU6xt50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "2F9J8ze-A_ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_dropout = EEG_CNN2(name=\"dp_0.3\")\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  with_dropout.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "batch_size=128\n",
        "train(with_dropout, balanced_train, balanced_val, batch_size=batch_size, num_epochs = 25)"
      ],
      "metadata": {
        "id": "c5Pj8ORGzzli",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "254a398c-c380-4ea6-d402-fb7849103b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available.  Training on CPU ...\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-70d54515cf76>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CUDA is not available.  Training on CPU ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalanced_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalanced_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-da69fa18dcab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, batch_size, num_epochs, learning_rate, MASTER_SAVE)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0maccuracyT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecallT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisionT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrixT\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracyT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0maccuracyV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecallV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisionV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrixV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5592b95f77ab>\u001b[0m in \u001b[0;36mget_metrics\u001b[0;34m(model, data, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c4df0a7fe07e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add batch norm after conv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add batch norm after conv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add conv3 and batch norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with_BN = EEG_CNN2(name=\"BN\")\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  with_BN.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "batch_size=128\n",
        "train(with_BN, balanced_train, balanced_val, batch_size=batch_size, num_epochs = 25)"
      ],
      "metadata": {
        "id": "4qBOsxShAQFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_BN2 = EEG_CNN3(name=\"BN\")\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  with_BN2.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "batch_size=256\n",
        "train(with_BN2, balanced_train, balanced_val, batch_size=batch_size, num_epochs = 25)"
      ],
      "metadata": {
        "id": "FyJbEFNfPJC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_BN_dropout = EEG_CNN2(name=\"BN_dp_0.3\")\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  with_BN_dropout.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "batch_size=128\n",
        "train(with_BN_dropout, balanced_train, balanced_val, batch_size=batch_size, num_epochs = 25)"
      ],
      "metadata": {
        "id": "P-RcySqtCf1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_CNN = EEG_CNN(name=\"default\")\n",
        "# if use_cuda and torch.cuda.is_available():\n",
        "#   cnn.cuda()\n",
        "torch.manual_seed(10)\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  default_CNN.cuda()\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "batch_size=128\n",
        "train(default_CNN, balanced_train, balanced_val, batch_size=batch_size, num_epochs = 25)"
      ],
      "metadata": {
        "id": "YV7r0S4gBApG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(10)\n",
        "# model = EEG_CNN()\n",
        "accuracy, recall, precision, f1, confusion_matrix = get_metrics(model,test_data, batch_size=128)\n",
        "print(accuracy)\n",
        "print(precision)\n",
        "print(recall)\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(confusion_matrix.cpu(), annot=True, cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('Actual Labels')\n",
        "plt.title('Confusion Matrix for Validation Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JpG0uyKTBDRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Standardize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Hyperparameter tuning\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "concatenated_data = []\n",
        "for i in range(len(loaded_data)):\n",
        "    features = []\n",
        "    for j in range(len(loaded_data[0][0])):\n",
        "        for k in range(len(loaded_data[0][0][0])):\n",
        "            features.append(loaded_data[i][0][j][k])\n",
        "    concatenated_data.append((features, loaded_data[i][1]))\n",
        "\n",
        "featuresSVM_train = []\n",
        "labelsSVM_train = []\n",
        "featuresSVM_test = []\n",
        "labelsSVM_test= []\n",
        "train_dataSVM, test_dataSVM = train_test_split(concatenated_data, stratify=binary_labels, test_size=0.3)\n",
        "\n",
        "for i in range(len(train_dataSVM)):\n",
        "    featuresSVM_train.append(train_dataSVM[i][0])\n",
        "    labelsSVM_train.append(train_dataSVM[i][1])\n",
        "\n",
        "for i in range(len(test_dataSVM)):\n",
        "    featuresSVM_test.append(test_dataSVM[i][0])\n",
        "    labelsSVM_test.append(test_dataSVM[i][1])\n",
        "\n",
        "# Create an SVM classifier\n",
        "clf = svm.SVC(kernel='poly') # Linear Kernel\n",
        "\n",
        "# Define the hyperparameter space to search over\n",
        "param_space = {\n",
        "    'C': uniform(loc=0, scale=10),\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "# Set up the k-fold cross-validation\n",
        "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
        "\n",
        "# Perform random search for hyperparameter tuning\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=clf,\n",
        "    param_distributions=param_space,\n",
        "    n_iter=10,  # Number of parameter settings that are sampled\n",
        "    cv=kfold,  # Cross-validation strategy\n",
        "    scoring='precision',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(featuresSVM_train, labelsSVM_train)\n",
        "\n",
        "# Retrieve the best hyperparameters and the corresponding model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the performance of the best model on the testing set\n",
        "labels_pred = best_model.predict(featuresSVM_test)\n",
        "accuracy = metrics.accuracy_score(labelsSVM_test, labels_pred)\n",
        "recall = metrics.recall_score(labelsSVM_test, labels_pred)\n",
        "# Print the best hyperparameters, precision, and accuracy\n",
        "print(f'The best hyperparameters are {best_params}')\n",
        "print(f'Precision on the testing dataset: {metrics.precision_score(labelsSVM_test, labels_pred):.4f}')\n",
        "print(f'Accuracy on the testing dataset: {accuracy:.4f}')\n",
        "print(f'Recall on the testing dataset: {recall:.4f}')\n"
      ],
      "metadata": {
        "id": "Fwqv8DQiaTT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Standardize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Hyperparameter tuning\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval\n",
        "\n",
        "concatenated_data = []\n",
        "for i in range(len(loaded_data)):\n",
        "    features = []\n",
        "    for j in range(len(loaded_data[0][0])):\n",
        "        for k in range(len(loaded_data[0][0][0])):\n",
        "            features.append(loaded_data[i][0][j][k])\n",
        "    concatenated_data.append((features,loaded_data[i][1]))\n",
        "\n",
        "\n",
        "featuresSVM_train = []\n",
        "labelsSVM_train = []\n",
        "featuresSVM_test = []\n",
        "labelsSVM_test= []\n",
        "train_dataSVM, test_dataSVM = train_test_split(concatenated_data, stratify=binary_labels, test_size=0.3)\n",
        "\n",
        "for i in range(len(train_dataSVM)):\n",
        "    featuresSVM_train.append(train_dataSVM[i][0])\n",
        "    labelsSVM_train.append(train_dataSVM[i][1])\n",
        "\n",
        "for i in range(len(test_dataSVM)):\n",
        "    featuresSVM_test.append(test_dataSVM[i][0])\n",
        "    labelsSVM_test.append(test_dataSVM[i][1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Create a svm Classifier\n",
        "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
        "#Train the model using the training sets\n",
        "clf.fit(featuresSVM_train, labelsSVM_train)\n",
        "#Predict the response for test dataset\n",
        "labels_pred = clf.predict(featuresSVM_test)\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy of tuned SVM:\",metrics.accuracy_score(labelsSVM_test, labels_pred))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision of tuned SVM:\",metrics.precision_score(labelsSVM_test, labels_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall of tuned SVM:\",metrics.recall_score(labelsSVM_test, labels_pred))"
      ],
      "metadata": {
        "id": "wTk9OYlPdiTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Standardize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Hyperparameter tuning\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval\n",
        "\n",
        "\n",
        "concatenated_data = []\n",
        "for i in range(len(loaded_data)):\n",
        "    features = []\n",
        "    for j in range(len(loaded_data[0][0])):\n",
        "        for k in range(len(loaded_data[0][0][0])):\n",
        "            features.append(loaded_data[i][0][j][k])\n",
        "    concatenated_data.append((features,loaded_data[i][1]))\n",
        "\n",
        "\n",
        "featuresSVM_train = []\n",
        "labelsSVM_train = []\n",
        "featuresSVM_test = []\n",
        "labelsSVM_test= []\n",
        "train_dataSVM, test_dataSVM = train_test_split(concatenated_data, stratify=binary_labels, test_size=0.3)\n",
        "\n",
        "for i in range(len(train_dataSVM)):\n",
        "    featuresSVM_train.append(train_dataSVM[i][0])\n",
        "    labelsSVM_train.append(train_dataSVM[i][1])\n",
        "\n",
        "for i in range(len(test_dataSVM)):\n",
        "    featuresSVM_test.append(test_dataSVM[i][0])\n",
        "    labelsSVM_test.append(test_dataSVM[i][1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Create a svm Classifier\n",
        "model = SVC(C=0.07066, gamma='scale', kernel='linear')\n",
        "#Train the model using the training sets\n",
        "model.fit(featuresSVM_train, labelsSVM_train)\n",
        "#Predict the response for test dataset\n",
        "labels_pred = model.predict(featuresSVM_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "from sklearn import metrics\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy of tuned SVM:\",metrics.accuracy_score(labelsSVM_test, labels_pred))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision of tuned SVM:\",metrics.precision_score(labelsSVM_test, labels_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall of tuned SVM:\",metrics.recall_score(labelsSVM_test, labels_pred))"
      ],
      "metadata": {
        "id": "MkbzRlmM6dFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Standardize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Hyperparameter tuning\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval\n",
        "\n",
        "concatenated_data = []\n",
        "for i in range(len(loaded_data)):\n",
        "    features = []\n",
        "    for j in range(len(loaded_data[0][0])):\n",
        "        for k in range(len(loaded_data[0][0][0])):\n",
        "            features.append(loaded_data[i][0][j][k])\n",
        "    concatenated_data.append((features,loaded_data[i][1]))\n",
        "\n",
        "\n",
        "featuresSVM_train = []\n",
        "labelsSVM_train = []\n",
        "featuresSVM_test = []\n",
        "labelsSVM_test= []\n",
        "train_dataSVM, test_dataSVM = train_test_split(concatenated_data, stratify=binary_labels, test_size=0.3)\n",
        "\n",
        "for i in range(len(train_dataSVM)):\n",
        "    featuresSVM_train.append(train_dataSVM[i][0])\n",
        "    labelsSVM_train.append(train_dataSVM[i][1])\n",
        "\n",
        "for i in range(len(test_dataSVM)):\n",
        "    featuresSVM_test.append(test_dataSVM[i][0])\n",
        "    labelsSVM_test.append(test_dataSVM[i][1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Create a svm Classifier\n",
        "clff = SVC(C=0.07066, gamma='scale', kernel='linear')\n",
        "#Train the model using the training sets\n",
        "clff.fit(featuresSVM_train, labelsSVM_train)\n",
        "#Predict the response for test dataset\n",
        "labels_pred = clff.predict(featuresSVM_test)\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy of tuned SVM:\",metrics.accuracy_score(labelsSVM_test, labels_pred))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision of tuned SVM:\",metrics.precision_score(labelsSVM_test, labels_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall of tuned SVM:\",metrics.recall_score(labelsSVM_test, labels_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGfVjRsT_k15",
        "outputId": "56c601ac-8e25-4a48-c442-500a714722b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of tuned SVM: 0.9831716191631291\n",
            "Precision of tuned SVM: 0.0\n",
            "Recall of tuned SVM: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "def trim_channels(lst, max_channels=23):\n",
        "    trimmed_lst = []\n",
        "    for data, label in lst:\n",
        "        # If data contains more than max_channels channels, trim it\n",
        "        if len(data) > max_channels:\n",
        "            data = data[:max_channels]\n",
        "        if len(data) == max_channels:\n",
        "            trimmed_lst.append((data, label))\n",
        "    return trimmed_lst\n",
        "\n",
        "loaded_data = trim_channels(loaded_data)\n",
        "print(len(loaded_data))\n",
        "# separate the True and False samples in train_data\n",
        "true_setSVM = [data for data in loaded_data if data[1] == True]\n",
        "false_setSVM = [data for data in loaded_data if data[1] == False]\n",
        "\n",
        "# oversample True class in train_data\n",
        "true_setSVM_oversampled = resample(true_setSVM, replace=True, n_samples=len(false_setSVM), random_state=123)\n",
        "# undersample False class in train_data\n",
        "false_setSVM_undersampled = resample(false_setSVM, replace=False, n_samples=len(true_setSVM_oversampled), random_state=123)\n",
        "\n",
        "# combine them to create a balanced train dataset\n",
        "balanced_SVM = true_setSVM_oversampled + false_setSVM_undersampled\n",
        "# shuffle the data\n",
        "random.shuffle(balanced_SVM)\n",
        "\n",
        "print(len(balanced_SVM))\n",
        "\n",
        "from sklearn import svm\n",
        "loaded_data = balanced_SVM\n",
        "concatenated_data = []\n",
        "for i in range(len(loaded_data)):\n",
        "    features = []\n",
        "    for j in range(len(loaded_data[0][0])):\n",
        "        for k in range(len(loaded_data[0][0][0])):\n",
        "            features.append(loaded_data[i][0][j][k])\n",
        "    concatenated_data.append((features,loaded_data[i][1]))\n",
        "\n",
        "\n",
        "featuresSVM_train = []\n",
        "labelsSVM_train = []\n",
        "featuresSVM_test = []\n",
        "labelsSVM_test= []\n",
        "\n",
        "stratify_labels = [data[1] for data in concatenated_data] # remove\n",
        "\n",
        "train_dataSVM, test_dataSVM = train_test_split(concatenated_data, stratify=stratify_labels, test_size=0.3) #(concatenated_data, stratify=binary_labels, test_size=0.3)\n",
        "# train_dataSVM, test_dataSVM = balanced_train, balanced_val\n",
        "for i in range(len(train_dataSVM)):\n",
        "    featuresSVM_train.append(train_dataSVM[i][0])\n",
        "    labelsSVM_train.append(train_dataSVM[i][1])\n",
        "\n",
        "for i in range(len(test_dataSVM)):\n",
        "    featuresSVM_test.append(test_dataSVM[i][0])\n",
        "    labelsSVM_test.append(test_dataSVM[i][1])\n",
        "\n",
        "#Create a svm Classifier\n",
        "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
        "#Train the model using the training sets\n",
        "# print(len(featuresSVM_train[0]))\n",
        "# print(len(labelsSVM_train))\n",
        "clf.fit(featuresSVM_train, labelsSVM_train)\n",
        "#Predict the response for test dataset\n",
        "labels_pred = clf.predict(featuresSVM_test)\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "# Model Accuracy: how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(labelsSVM_test, labels_pred))\n",
        "\n",
        "# Model Precision: what percentage of positive tuples are labeled as such?\n",
        "print(\"Precision:\",metrics.precision_score(labelsSVM_test, labels_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(labelsSVM_test, labels_pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(labelsSVM_test, labels_pred)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZK5qFvjZ-Tk0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}